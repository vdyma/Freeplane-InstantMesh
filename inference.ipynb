{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from src.utils.train_util import instantiate_from_config\n",
    "import rembg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from src.utils.infer_util import remove_background, resize_foreground, save_video\n",
    "from einops import rearrange\n",
    "from src.utils.camera_util import get_zero123plus_input_cameras\n",
    "from torchvision.transforms import v2\n",
    "from kornia.filters import bilateral_blur\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"configs/instant-mesh-base.yaml\")\n",
    "config_name = \"instant-mesh-base\"\n",
    "model_config = config.model_config\n",
    "infer_config = config.infer_config\n",
    "\n",
    "IS_FLEXICUBES = True\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"out\"\n",
    "input_path = \"examples/cartoon_dinosaur.png\"\n",
    "\n",
    "# make output directories\n",
    "image_path = os.path.join(output_path, config_name, 'images')\n",
    "mesh_path = os.path.join(output_path, config_name, 'meshes')\n",
    "video_path = os.path.join(output_path, config_name, 'videos')\n",
    "os.makedirs(image_path, exist_ok=True)\n",
    "os.makedirs(mesh_path, exist_ok=True)\n",
    "os.makedirs(video_path, exist_ok=True)\n",
    "\n",
    "# process input files\n",
    "if os.path.isdir(input_path):\n",
    "    input_files = [\n",
    "        os.path.join(input_path, file) \n",
    "        for file in os.listdir(input_path) \n",
    "        if file.endswith('.png') or file.endswith('.jpg') or file.endswith('.webp')\n",
    "    ]\n",
    "else:\n",
    "    input_files = [input_path]\n",
    "print(f'Total number of input images: {len(input_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"sudo-ai/zero123plus-v1.2\", \n",
    "    custom_pipeline=\"zero123plus\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "    pipeline.scheduler.config, timestep_spacing='trailing'\n",
    ")\n",
    "if os.path.exists(infer_config.unet_path):\n",
    "    unet_ckpt_path = infer_config.unet_path\n",
    "else:\n",
    "    unet_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=\"diffusion_pytorch_model.bin\", repo_type=\"model\")\n",
    "state_dict = torch.load(unet_ckpt_path, map_location='cpu')\n",
    "pipeline.unet.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "pipeline = pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rembg = True\n",
    "diffusion_steps = 75\n",
    "\n",
    "rembg_session = None if no_rembg else rembg.new_session()\n",
    "\n",
    "outputs = []\n",
    "for idx, image_file in enumerate(input_files):\n",
    "    name = os.path.basename(image_file).split('.')[0]\n",
    "    print(f'[{idx+1}/{len(input_files)}] Imagining {name} ...')\n",
    "\n",
    "    # remove background optionally\n",
    "    input_image = Image.open(image_file)\n",
    "    if not no_rembg:\n",
    "        input_image = remove_background(input_image, rembg_session)\n",
    "        input_image = resize_foreground(input_image, 0.85)\n",
    "    \n",
    "    # sampling\n",
    "    output_image = pipeline(\n",
    "        input_image, \n",
    "        num_inference_steps=diffusion_steps, \n",
    "    ).images[0]\n",
    "\n",
    "    output_image.save(os.path.join(image_path, f'{name}.png'))\n",
    "    print(f\"Image saved to {os.path.join(image_path, f'{name}.png')}\")\n",
    "\n",
    "    images = np.asarray(output_image, dtype=np.float32) / 255.0\n",
    "    images = torch.from_numpy(images).permute(2, 0, 1).contiguous().float()     # (3, 960, 640)\n",
    "    images = rearrange(images, 'c (n h) (m w) -> (n m) c h w', n=3, m=2)        # (6, 3, 320, 320)\n",
    "\n",
    "    outputs.append({'name': name, 'images': images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete pipeline to save memory\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_from_config(model_config)\n",
    "if os.path.exists(infer_config.model_path):\n",
    "    model_ckpt_path = infer_config.model_path\n",
    "else:\n",
    "    model_ckpt_path = hf_hub_download(repo_id=\"TencentARC/InstantMesh\", filename=f\"{config_name.replace('-', '_')}.ckpt\", repo_type=\"model\")\n",
    "state_dict = torch.load(model_ckpt_path, map_location='cpu')['state_dict']\n",
    "state_dict = {k[14:]: v for k, v in state_dict.items() if k.startswith('lrm_generator.')}\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "model = model.to(device)\n",
    "if IS_FLEXICUBES:\n",
    "    model.init_flexicubes_geometry(device, fovy=30.0)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1.0\n",
    "view = 6\n",
    "export_texmap = True\n",
    "do_save_video = True\n",
    "distance = 4.5\n",
    "\n",
    "name = os.path.basename(input_files[0]).split('.')[0]\n",
    "img_path = os.path.join(image_path, f'{name}.png')\n",
    "img = Image.open(img_path)\n",
    "images = np.asarray(img, dtype=np.float32) / 255.0\n",
    "images = torch.from_numpy(images).permute(2, 0, 1).contiguous().float()     # (3, 960, 640)\n",
    "images = rearrange(images, 'c (n h) (m w) -> (n m) c h w', n=3, m=2)    \n",
    "outputs = [{'name': name, 'images': images}]\n",
    "\n",
    "input_cameras = get_zero123plus_input_cameras(batch_size=1, radius=4.0*scale).to(device)\n",
    "chunk_size = 20 if IS_FLEXICUBES else 1\n",
    "\n",
    "for idx, sample in enumerate(outputs):\n",
    "    name = sample['name']\n",
    "    print(f'[{idx+1}/{len(outputs)}] Creating {name} ...')\n",
    "\n",
    "    images = sample['images'].unsqueeze(0).to(device)\n",
    "    images = v2.functional.resize(images, 320, interpolation=3, antialias=True).clamp(0, 1)\n",
    "\n",
    "    if view == 4:\n",
    "        indices = torch.tensor([0, 2, 4, 5]).long().to(device)\n",
    "        images = images[:, indices]\n",
    "        input_cameras = input_cameras[:, indices]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # get triplane\n",
    "        planes = model.forward_planes(images, input_cameras)\n",
    "        \n",
    "        # Get freeplanes\n",
    "        k = (3, 3)\n",
    "        sigma_c = 10\n",
    "        sigma_r = (10, 10)\n",
    "    \n",
    "        freeplanes = bilateral_blur(\n",
    "            planes.transpose(0, 2).squeeze(), k, sigma_c, sigma_r\n",
    "        ).unsqueeze(2).transpose(0, 2)\n",
    "\n",
    "        # get mesh\n",
    "        mesh_path_idx = os.path.join(mesh_path, f'{name}.obj')\n",
    "\n",
    "        mesh_out = model.extract_mesh(\n",
    "            planes,\n",
    "            use_texture_map=export_texmap,\n",
    "            freeplanes=freeplanes,\n",
    "            **infer_config,\n",
    "        )\n",
    "        if export_texmap:\n",
    "            vertices, faces, uvs, mesh_tex_idx, tex_map = mesh_out\n",
    "            run.save_obj_with_mtl(\n",
    "                vertices.data.cpu().numpy(),\n",
    "                uvs.data.cpu().numpy(),\n",
    "                faces.data.cpu().numpy(),\n",
    "                mesh_tex_idx.data.cpu().numpy(),\n",
    "                tex_map.permute(1, 2, 0).data.cpu().numpy(),\n",
    "                mesh_path_idx,\n",
    "            )\n",
    "        else:\n",
    "            vertices, faces, vertex_colors = mesh_out\n",
    "            run.save_obj(vertices, faces, vertex_colors, mesh_path_idx)\n",
    "        print(f\"Mesh saved to {mesh_path_idx}\")\n",
    "\n",
    "        # get video\n",
    "        if do_save_video:\n",
    "            video_path_idx = os.path.join(video_path, f'{name}.mp4')\n",
    "            render_size = infer_config.render_resolution\n",
    "            render_cameras = run.get_render_cameras(\n",
    "                batch_size=1, \n",
    "                M=120, \n",
    "                radius=distance, \n",
    "                elevation=20.0,\n",
    "                is_flexicubes=IS_FLEXICUBES,\n",
    "            ).to(device)\n",
    "            \n",
    "            frames = run.render_frames(\n",
    "                model, \n",
    "                planes, \n",
    "                render_cameras=render_cameras, \n",
    "                render_size=render_size, \n",
    "                chunk_size=chunk_size, \n",
    "                is_flexicubes=IS_FLEXICUBES,\n",
    "            )\n",
    "\n",
    "            save_video(\n",
    "                frames,\n",
    "                video_path_idx,\n",
    "                fps=30,\n",
    "            )\n",
    "            print(f\"Video saved to {video_path_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
